{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n",
      "[[181, 208], [208, 251]]\n",
      "[[181, 208], [208, 251], [881, 892], [892, 901]]\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "datanum = 26 ##number of rows on the abundance table, modify as needed\n",
    "ndigits= len(str(datanum))\n",
    "raynum = 4 ##number of rays used, modify as needed\n",
    "\n",
    "for r in range(raynum):\n",
    "    rowlist = []\n",
    "    for i in range(datanum):\n",
    "        m = i+1\n",
    "        n_len = len(str(m))\n",
    "        n_zeros = ndigits - n_len\n",
    "        k = \"0\" * n_zeros + str(m)\n",
    "        row_data = pd.read_csv(path+f\"data_AbundanceRow{k}_C_IV.txt\", delim_whitespace=True) ##read in data files\n",
    "        row_work = row_data[row_data[\"lightray_index\"]==r] ##filter to only ray1\n",
    "        df = row_work[[\"vel_dispersion\",\"interval_start\",\"interval_end\"]].reset_index().drop(columns=\"index\") ##filter to only indexes and velocites\n",
    "        rowlist.append(df)\n",
    "        \n",
    "\n",
    "    mx = -np.inf\n",
    "    for ds in rowlist:\n",
    "        row_mx = max(ds[\"interval_end\"])\n",
    "        if row_mx > mx:\n",
    "            mx = row_mx\n",
    "\n",
    "    super_clumps = np.zeros(int(mx))\n",
    "\n",
    "    clmaps = []\n",
    "    for ds in rowlist:\n",
    "        ds_clump_loc = np.zeros(int(mx))\n",
    "        for i in range(ds.shape[0]):\n",
    "            ds_clump_loc[int(ds['interval_start'][i]):int(ds['interval_end'][i])] = 1\n",
    "            super_clumps[int(ds['interval_start'][i]):int(ds['interval_end'][i])] = 1\n",
    "        clmaps.append(ds_clump_loc)\n",
    "\n",
    "    super_clumps = np.append(0,super_clumps)\n",
    "    super_clumps = np.append(super_clumps,0)\n",
    "    \n",
    "    \n",
    "    match = {} ##create dictionaries to store indexes of clumps that correspond to one another\n",
    "    shorter = {}\n",
    "    merge = {}\n",
    "    lonely = {}\n",
    "\n",
    "    maybe_lonely = {}\n",
    "\n",
    "    rownum = 0\n",
    "\n",
    "    for row in clmaps: \n",
    "\n",
    "        row = np.append(0,row) # adding an extra element to prevent booleans from failing\n",
    "        row = np.append(row,0)\n",
    "        rownum += 1\n",
    "\n",
    "        row_st_cnt = 0\n",
    "        row_st_ind = []\n",
    "\n",
    "        row_en_cnt = 0\n",
    "        row_en_ind = []\n",
    "\n",
    "        row_match = []\n",
    "        row_short = []\n",
    "        row_merge = []\n",
    "\n",
    "\n",
    "\n",
    "        for i in range(1,len(row)):\n",
    "\n",
    "            if super_clumps[i-1]<super_clumps[i]:\n",
    "                sup_st = i-1\n",
    "\n",
    "            if row[i-1]<row[i]:\n",
    "                row_st_cnt += 1\n",
    "                row_st_ind.append(i-1)\n",
    "\n",
    "            elif row[i-1]>row[i]:\n",
    "                row_en_cnt += 1\n",
    "                row_en_ind.append(i-1)\n",
    "\n",
    "            if super_clumps[i-1]>super_clumps[i]:\n",
    "                sup_en = i-1\n",
    "\n",
    "                if (row_st_cnt == 1) & (row_en_cnt == 1):\n",
    "                    if (row_st_ind[0] == sup_st) & (row_en_ind[0] == sup_en):\n",
    "                        row_match.append([row_st_ind[0],row_en_ind[0]])\n",
    "\n",
    "                    else:\n",
    "                        row_short.append([row_st_ind[0],row_en_ind[0]])\n",
    "\n",
    "                elif (row_st_cnt == 0) & (row_en_cnt == 0):\n",
    "\n",
    "                    if str([sup_st,sup_en]) in maybe_lonely.keys():\n",
    "                        maybe_lonely[str([sup_st,sup_en])] += 1\n",
    "\n",
    "                    else:\n",
    "                        maybe_lonely[str([sup_st,sup_en])] = 1\n",
    "\n",
    "                else:\n",
    "\n",
    "                    for j in range(len(row_st_ind)):\n",
    "                        row_merge.append([row_st_ind[j],row_en_ind[j]]) \n",
    "\n",
    "                row_st_cnt = 0\n",
    "                row_st_ind = []\n",
    "\n",
    "                row_en_cnt = 0\n",
    "                row_en_ind = []\n",
    "\n",
    "            match[rownum] = row_match\n",
    "            shorter[rownum] = row_short\n",
    "            merge[rownum] = row_merge\n",
    "\n",
    "    false_merge = {}\n",
    "\n",
    "    for i in range(rownum):\n",
    "        for cat in [shorter, match, merge]:\n",
    "            true_spans = []\n",
    "\n",
    "            for n in range(len(rowlist[i])):\n",
    "                true_spans.append([rowlist[i][\"interval_start\"][n],rowlist[i][\"interval_end\"][n].astype(int)])\n",
    "\n",
    "            false_clumps = []\n",
    "\n",
    "            for span in cat[i+1]:        \n",
    "\n",
    "                if span in true_spans:\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    true_span_arr = np.asarray(true_spans)\n",
    "\n",
    "\n",
    "                    sp_st = span[0]\n",
    "                    sp_end = span[1]\n",
    "\n",
    "                    n = np.where(sp_st == true_span_arr[:,0])[0]\n",
    "                    p = 0\n",
    "\n",
    "                    while sp_end >= true_span_arr[n+p,1]:\n",
    "\n",
    "                        false_clumps.append([int(true_span_arr[n+p,0]),int(true_span_arr[n+p,1])])\n",
    "                        p+=1\n",
    "\n",
    "                        if (n+p)>= len(true_span_arr[:,1]):\n",
    "                            break\n",
    "\n",
    "#                     cat[i+1].remove(span)\n",
    "\n",
    "#                     false_merge[i+1] = false_clumps\n",
    "                    \n",
    "                    \n",
    "                if i+1 in false_merge.keys():\n",
    "                    false_merge[i+1].append(false_clumps[0])\n",
    "                    false_merge[i+1].append(false_clumps[1])\n",
    "                else:\n",
    "                    false_merge[i+1] = false_clumps\n",
    "                cat[i+1].remove(span) ##modify og dictionaries to help later \n",
    "                    \n",
    "                    \n",
    "#     print(false_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "path = \"./data/\"\n",
    "datanum = 26 ##number of rows on the abundance table, modify as needed\n",
    "ndigits= len(str(datanum))\n",
    "raynum = 4 ##number of rays used, modify as needed\n",
    "\n",
    "for r in range(raynum):\n",
    "    r = 1\n",
    "    rowlist = []\n",
    "    for i in range(datanum):\n",
    "        m = i+1\n",
    "        n_len = len(str(m))\n",
    "        n_zeros = ndigits - n_len\n",
    "        k = \"0\" * n_zeros + str(m)\n",
    "        row_data = pd.read_csv(path+f\"data_AbundanceRow{k}_C_IV.txt\", delim_whitespace=True) ##read in data files\n",
    "        row_work = row_data[row_data[\"lightray_index\"]==r] ##filter to only ray1\n",
    "        df = row_work[[\"interval_start\",\"interval_end\"]].reset_index().drop(columns=\"index\") ##filter to only indexes\n",
    "        rowlist.append(df)\n",
    "\n",
    "    mx= -np.inf  ##find how long each array should be\n",
    "    for ds in rowlist: #find the cell index of the furthest clump\n",
    "        row_mx = max(ds[\"interval_end\"])\n",
    "        if row_mx>mx:\n",
    "            mx=row_mx\n",
    "    super_clumps = np.zeros(int(mx))\n",
    "    clmaps = []\n",
    "    for ds in rowlist: ##make masks for each row and form super_clumps\n",
    "        ds_clump_loc = np.zeros(int(mx))\n",
    "        for i in range(1, ds.shape[0]):\n",
    "            ds_clump_loc[int(ds[\"interval_start\"][i]):int(ds[\"interval_end\"][i])] = 1\n",
    "            super_clumps[int(ds[\"interval_start\"][i]):int(ds[\"interval_end\"][i])] = 1\n",
    "        clmaps.append(ds_clump_loc)\n",
    "        super_clumps=np.append(0, super_clumps)  ##make indexing work  \n",
    "    super_clumps=np.append(super_clumps, 0)\n",
    "    np.save(f'super_clumps_array_ray{r}', super_clumps)\n",
    "                    \n",
    "    match = {} ##create dictionaries to store indexes of clumps in the row that correspond to one another, keys will be row numbers and values will be indecies except for the lonlies\n",
    "    short = {}\n",
    "    merge = {}\n",
    "    false_merge ={}\n",
    "    lonely = {}\n",
    "    maybe_lonely = {}\n",
    "    rownum = 0\n",
    "\n",
    "    for row in clmaps:  ##start by iterating over a whole row\n",
    "        row = np.append(0,row) # adding an extra element to prevent booleans from failing\n",
    "        row = np.append(row,0)\n",
    "        rownum += 1 ##define which row we're working on\n",
    "    ##make all the variables and lists necessary\n",
    "        row_st_cnt = 0 ##count how many starts of row clumps there have been within a super clump\n",
    "        row_st_ind = []  ##keep track of start location(s) of a row clump\n",
    "\n",
    "        row_en_cnt = 0 ##how many ends of row clumps there have been within a super clump\n",
    "        row_en_ind = [] ##keep track of end location(s) of a row clump\n",
    "        row_match = []\n",
    "        row_short = []\n",
    "        row_merge = []\n",
    "    \n",
    "        for i in range(1,len(row)):\n",
    "            if super_clumps[i-1]<super_clumps[i]: ##start of a super clump\n",
    "                sup_st = i-1 ##keep track of start location of a super clump\n",
    "            if row[i-1]<row[i]:  ##start of a clump in the row\n",
    "                row_st_cnt += 1\n",
    "                row_st_ind.append(i-1)\n",
    "        \n",
    "            elif row[i-1]>row[i]: ##end of a clump in row\n",
    "                row_en_cnt += 1\n",
    "                row_en_ind.append(i-1)\n",
    "            \n",
    "            if super_clumps[i-1]>super_clumps[i]: ##end of a super clump\n",
    "                sup_en = i-1 ##keep track of the location of the end of a super clump\n",
    "          \n",
    "                if (row_st_cnt == 1) and (row_en_cnt == 1): ##check for if there is only one row clump in the super clump\n",
    "    \n",
    "                    if (row_st_ind[0] == sup_st) & (row_en_ind[0] == sup_en): ##if the starts and ends match, the clumps are identical\n",
    "                        row_match.append([row_st_ind[0],row_en_ind[0]]) ##thus, start and end indecies appended to a list of them\n",
    "                    else:\n",
    "                        row_short.append([row_st_ind[0],row_en_ind[0]]) ##if not, then the row clump must be shorter and the start and end indecies are appended to the appropraite list\n",
    "\n",
    "                elif (row_st_cnt == 0) & (row_en_cnt == 0): ##check if there is nothing in the row that matches the super clump\n",
    "                \n",
    "                    if str([sup_st,sup_en]) in maybe_lonely.keys(): ##check if we have already seen this superclump, if not make the entry in the dictionary\n",
    "                        maybe_lonely[str([sup_st,sup_en])] += 1\n",
    "                    else:\n",
    "                        maybe_lonely[str([sup_st,sup_en])] = 1\n",
    "                else: ##only other senario is there there was a merge\n",
    "                    for j in range(len(row_st_ind)): ##organize the indecies to make the list in order\n",
    "                        row_merge.append([row_st_ind[j],row_en_ind[j]]) \n",
    "                                 \n",
    "                row_st_cnt = 0\n",
    "                row_st_ind = []\n",
    "                row_en_cnt = 0\n",
    "                row_en_ind = []\n",
    "                match[rownum] = row_match\n",
    "                short[rownum] = row_short\n",
    "                merge[rownum] = row_merge\n",
    "\n",
    "    for h in range(rownum):\n",
    "        for dic in [match, short, merge]: ##iterate over all the dictionaries\n",
    "            true_spans = [] ##array of indecies where clumps really are\n",
    "            \n",
    "            for n in range(len(rowlist[h])):\n",
    "                true_spans.append([rowlist[h][\"interval_start\"][n], rowlist[h][\"interval_end\"][n].astype(int)])\n",
    "                \n",
    "            false_clumps = []\n",
    "            \n",
    "            for span in dic[h+1]:\n",
    "\n",
    "                if span in true_spans: ##pass by if it really is a clump\n",
    "                    continue\n",
    "                    \n",
    "                else: ##some weird merge happened so we have to fix it\n",
    "                    true_span_arr = np.array(true_spans)\n",
    "                    sp_st = span[0]\n",
    "                    sp_en = span[1]\n",
    "                    n = int(np.where(sp_st == true_span_arr[:,0])[0])\n",
    "                    p = 0\n",
    "                    \n",
    "                    while sp_en >= true_span_arr[n+p, 1]:\n",
    "                        false_clumps.append(int(true_span_arr[n+p, 0]))\n",
    "                        false_clumps.append(int(true_span_arr[n+p, 1])) ##make it so we have the start and end indecies\n",
    "                        p += 1\n",
    "                        \n",
    "                        if (n+p) >= len(true_span_arr[:,1]):\n",
    "                            break\n",
    "                            \n",
    "                if h+1 in false_merge.keys():\n",
    "                    false_merge[h+1].append(false_clumps)\n",
    "                    \n",
    "                else:\n",
    "                    false_merge[h+1] = [false_clumps]\n",
    "                dic[h+1].remove(span) ##modify og dictionaries to help later\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 2: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 3: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 4: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 5: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 6: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 7: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 8: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 9: [[390, 425], [934, 940], [1102, 1138], [1148, 1162]], 10: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 11: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 12: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 13: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 14: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 15: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 16: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 17: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 18: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 19: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 20: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 21: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 22: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 23: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 24: [[390, 425], [934, 940], [1102, 1138], [1148, 1162]], 25: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]], 26: [[390, 425], [442, 785], [795, 873], [905, 920], [934, 937], [973, 986], [988, 999], [1000, 1011], [1102, 1136], [1149, 1162]]}\n"
     ]
    }
   ],
   "source": [
    "# print(maybe_lonely)\n",
    "# print(shorter)\n",
    "# print(match)\n",
    "print(merge)\n",
    "# print(false_merge[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1_st = np.asarray(df1_clumps[\"interval_start\"])\n",
    "# df2_st = np.asarray(df2_clumps[\"interval_start\"])\n",
    "# df1_en = np.asarray(df1_clumps[\"interval_end\"])\n",
    "# df2_en = np.asarray(df2_clumps[\"interval_end\"])\n",
    "\n",
    "\n",
    "match = {} ##create dictionaries to store indexes of clumps that correspond to one another\n",
    "shorter = {}\n",
    "merge = {}\n",
    "lonely = {}\n",
    "\n",
    "maybe_lonely = {}\n",
    "\n",
    "rownum = 0\n",
    "\n",
    "for row in clmaps: \n",
    "\n",
    "    row = np.append(0,row) # adding an extra element to prevent booleans from failing\n",
    "    row = np.append(row,0)\n",
    "    rownum += 1\n",
    "\n",
    "    row_st_cnt = 0\n",
    "    row_st_ind = []\n",
    "    \n",
    "    row_en_cnt = 0\n",
    "    row_en_ind = []\n",
    "\n",
    "    row_match = []\n",
    "    row_short = []\n",
    "    row_merge = []\n",
    "    \n",
    " \n",
    "    \n",
    "    for i in range(1,len(row)):\n",
    "        \n",
    "        if super_clumps[i-1]<super_clumps[i]:\n",
    "            sup_st = i-1\n",
    "            \n",
    "        if row[i-1]<row[i]:\n",
    "            row_st_cnt += 1\n",
    "            row_st_ind.append(i-1)\n",
    "        \n",
    "        elif row[i-1]>row[i]:\n",
    "            row_en_cnt += 1\n",
    "            row_en_ind.append(i-1)\n",
    "            \n",
    "        if super_clumps[i-1]>super_clumps[i]:\n",
    "            sup_en = i-1\n",
    "            \n",
    "            if (row_st_cnt == 1) & (row_en_cnt == 1):\n",
    "                if (row_st_ind[0] == sup_st) & (row_en_ind[0] == sup_en):\n",
    "                    row_match.append([row_st_ind[0],row_en_ind[0]])\n",
    "            \n",
    "                else:\n",
    "                    row_short.append([row_st_ind[0],row_en_ind[0]])\n",
    "            \n",
    "            elif (row_st_cnt == 0) & (row_en_cnt == 0):\n",
    "                \n",
    "                if str([sup_st,sup_en]) in maybe_lonely.keys():\n",
    "                    maybe_lonely[str([sup_st,sup_en])] += 1\n",
    "                                 \n",
    "                else:\n",
    "                    maybe_lonely[str([sup_st,sup_en])] = 1\n",
    "            \n",
    "            else:\n",
    "            \n",
    "                for j in range(len(row_st_ind)):\n",
    "                    row_merge.append([row_st_ind[j],row_en_ind[j]]) \n",
    "                                 \n",
    "            row_st_cnt = 0\n",
    "            row_st_ind = []\n",
    "            \n",
    "            row_en_cnt = 0\n",
    "            row_en_ind = []\n",
    "        \n",
    "        match[rownum] = row_match\n",
    "        shorter[rownum] = row_short\n",
    "        merge[rownum] = row_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[1745, 1763]': 1}\n",
      "{1: [[164, 174], [181, 251], [345, 358], [934, 937], [1102, 1136], [1149, 1162], [1197, 1206], [1550, 1562]], 2: []}\n",
      "{1: [[390, 425], [1242, 1252], [1467, 1475]], 2: [[160, 174], [178, 251], [344, 359], [390, 425], [442, 929], [934, 940], [971, 1011], [1102, 1138], [1148, 1162], [1197, 1210], [1242, 1252], [1467, 1475], [1548, 1564], [1745, 1763]]}\n",
      "{1: [[442, 785], [795, 873], [881, 901], [905, 920], [973, 986], [988, 999], [1000, 1011]], 2: []}\n"
     ]
    }
   ],
   "source": [
    "print(maybe_lonely)\n",
    "print(shorter)\n",
    "print(match)\n",
    "print(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_merge = {}\n",
    "\n",
    "for i in range(rownum):\n",
    "    for cat in [shorter, match, merge]:\n",
    "        true_spans = []\n",
    "        \n",
    "        for n in range(len(rowlist[i])):\n",
    "            true_spans.append([rowlist[i][\"interval_start\"][n],rowlist[i][\"interval_end\"][n].astype(int)])\n",
    "\n",
    "        false_clumps = []\n",
    "\n",
    "        for span in cat[i+1]:        \n",
    "\n",
    "            if span in true_spans:\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                true_span_arr = np.asarray(true_spans)\n",
    "                \n",
    "                \n",
    "                sp_st = span[0]\n",
    "                sp_end = span[1]\n",
    "\n",
    "                n = np.where(sp_st == true_span_arr[:,0])[0]\n",
    "                p = 0\n",
    "\n",
    "                while sp_end >= true_span_arr[n+p,1]:\n",
    "\n",
    "                    false_clumps.append([int(true_span_arr[n+p,0]),int(true_span_arr[n+p,1])])\n",
    "                    p+=1\n",
    "\n",
    "                    if (n+p)>= len(true_span_arr[:,1]):\n",
    "                        break\n",
    "                \n",
    "                cat[i+1].remove(span)\n",
    "                \n",
    "                false_merge[i+1] = false_clumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [[881, 892], [892, 901]], 2: [[442, 881], [881, 929]]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[1745, 1763]': 1}\n",
      "{1: [[164, 174], [345, 358], [934, 937], [1102, 1136], [1149, 1162], [1197, 1206], [1550, 1562]], 2: []}\n",
      "{1: [[390, 425], [1242, 1252], [1467, 1475]], 2: [[160, 174], [178, 251], [344, 359], [390, 425], [934, 940], [971, 1011], [1102, 1138], [1148, 1162], [1197, 1210], [1242, 1252], [1467, 1475], [1548, 1564], [1745, 1763]]}\n",
      "{1: [[442, 785], [795, 873], [905, 920], [973, 986], [988, 999], [1000, 1011]], 2: []}\n"
     ]
    }
   ],
   "source": [
    "print(maybe_lonely)\n",
    "print(shorter)\n",
    "print(match)\n",
    "print(merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 160,  178,  344,  390,  442,  881,  934,  971, 1102, 1148, 1197,\n",
       "       1242, 1467, 1548, 1745])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_span_arr = np.asarray(true_spans)\n",
    "\n",
    "true_span_arr[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
